---
title: "eda2.rmd"
author: "Daniel Barlow"
date: "November 7, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(readxl)
library(randomForest)
library(glmnet)
df <- read.csv("Final_Game_Data.csv")
Predictions <- read.csv("Predictions.csv")
Predictions.test <- read.csv("Predictions_test.csv")
```

```{r}
df$Home_balance <- df$Home_RushAtt/(df$Home_RushAtt + df$Home_PassAtt)
df$Away_balance <- df$Away_RushAtt/(df$Away_RushAtt + df$Away_PassAtt)
df <- na.omit(df)
df$Home.Team <- as.character(df$Home.Team)
df$Visitor.Team <- as.character(df$Visitor.Team)
Predictions$Home.Team <- as.character(Predictions$Home.Team)
Predictions$Visitor.Team <- as.character(Predictions$Visitor.Team)
Predictions$Home.Team[Predictions$Home.Team == "Pittsburgh"] <- "Pitt"
Predictions$Visitor.Team[Predictions$Visitor.Team == "Pittsburgh"] <- "Pitt"
```

```{r}
set.seed(1305)
size = floor(.7*nrow(df))
train.idx = sample(seq_len(nrow(df)), size = size)
train = df[train.idx, -c(1:4, 24, 25, 44, 45:49)] #takes out any variables that have characters associated to them as well as spread, home.score, visitor.score, total
test = df[-train.idx, -c(1:4, 24, 25, 44, 45:49)]
```

GLM
```{r}
glm.test = df[-train.idx, -c(1:4, 25, 45:49)]
glm.test$Home_X.1 <- ifelse(test$Home_X.1 == "W", 1, 0)

glm.model = glm(Home_X.1 ~ ., data = train, family = "binomial", control = list(maxit = 50))
summary(glm.model)
coef(glm.model)
probs.full = glm.model %>% 
  predict(Predictions.test, type = "response")
predicted.classes <- ifelse(probs.full > 0.5, "1", "0")
```

The most commonly used penalized regression include:

ridge regression: variables with minor contribution have their coefficients close to zero. However, all the variables are incorporated in the model. This is useful when all variables need to be incorporated in the model according to domain knowledge.
```{r}
ridge.test = df[-train.idx, -c(1:4, 25, 45:49)]
ridge.test$Home_X.1 <- ifelse(test$Home_X.1 == "W", 1, 0)

x.train <- model.matrix(Home_X.1~., train)[, -1]
y.train <- ifelse(train$Home_X.1 == "W", 1, 0)
x.test <- model.matrix(Home_X.1 ~., ridge.test)[, -1]
y.test <- ifelse(ridge.test$Home_X.1 == "W", 1, 0)
ridge.lambda =cv.glmnet(x.train, y.train, alpha = 0)
plot(ridge.lambda)
ridge.lambda$lambda.min
ridge.model = glmnet(x.train, y.train, family = "binomial", alpha = 0, lambda = ridge.lambda$lambda.min)
probs.ridge <- ridge.model %>% 
  predict(s='ridge.lambda$lambda.min', newx=data.matrix(Predictions.test[, -c(1:7)]), type="response")
quantile(probs.ridge)
predicted.classes <- ifelse(probs.ridge > 0.5, "1", "0")
```

lasso regression: the coefficients of some less contributive variables are forced to be exactly zero. Only the most significant variables are kept in the final model.
```{r}
set.seed(1305)
lasso.test = df[-train.idx, -c(1:4, 25, 45:49)]
lasso.test$Home_X.1 <- ifelse(test$Home_X.1 == "W", 1, 0)
#build model matrixes
x.train <- model.matrix(Home_X.1~., train)[, -1]
y.train <- ifelse(train$Home_X.1 == "W", 1, 0)
x.test <- model.matrix(Home_X.1 ~., lasso.test)[, -1]
y.test <- ifelse(lasso.test$Home_X.1 == "W", 1, 0)
lasso.lambda =cv.glmnet(x.train, y.train, family = "binomial", alpha = 1)
# plot(lasso.lambda)
lasso.lambda$lambda.min
lasso.model = glmnet(x.train, y.train, family = "binomial", alpha = 1, lambda = lasso.lambda$lambda.min)
# coef(lasso.lambda)
probs.lasso <- lasso.model %>% 
  predict(s='lasso.lambda$lambda.min', newx=data.matrix(Predictions.test[, -c(1:7)]), type="response")
(predicted.classes <- ifelse(probs.lasso > 0.5, "1", "0"))
```

Random Forest
```{r}
train.rf.class <- df[train.idx, -c(1:4, 24, 25, 44, 45:49)]
test.rf.class <-  df[-train.idx, -c(1:4, 24, 25, 44, 45:49)]

rf.model = randomForest(x=dplyr::select(train.rf.class,-Home_X.1),
                     y=as.factor(train.rf.class$Home_X.1),
                     xtest=dplyr::select(test.rf.class,-Home_X.1),
                     ytest=as.factor(test.rf.class$Home_X.1),
                     type="classification", keep.forest = T)

layout(matrix(c(1,2),nrow=1),
       width=c(4,1)) 
par(mar=c(5,4,4,0)) #No margin on the right side
plot(rf.model, log="y")
par(mar=c(5,0,4,2)) #No margin on the left side
plot(c(0,1),type="n", axes=F, xlab="", ylab="")
legend("top", colnames(rf.model$err.rate),col=1:4,cex=0.8,fill=1:4)

rf.model$test$confusion
predict(rf.model, newdata=data.matrix(Predictions.test[, -c(1:7)]))
importance(rf.model)
```
 


Score


Linear
```{r}
Predictions.test$Home_balance <- Predictions.test$Home_RushAtt/(Predictions.test$Home_RushAtt + Predictions.test$Home_PassAtt)
Predictions.test$Away_balance <- Predictions.test$Away_RushAtt/(Predictions.test$Away_RushAtt + Predictions.test$Away_PassAtt)
  
train.Home.score = df[train.idx, -c(1, 3, 5, 24, 25, 44, 46:49)]
test.Home.score = df[-train.idx, -c(1, 3, 5, 24, 25, 44, 46:49)]
train.Visitor.score = df[train.idx, -c(1, 3, 5, 24, 25, 44, 45, 47:49)]
test.Visitor.score = df[-train.idx, -c(1, 3, 5, 24, 25, 44, 45, 47:49)]

lm1.Home.score <- lm(Home.Score ~ ., data = train.Home.score)
step.home.score <- step(lm1.Home.score, trace = 0)

lm2.Home.score <- lm(formula(step.home.score), data = train.Home.score)
summary(lm2.Home.score)
predict(lm2.Home.score, newdata = test.Home.score)
modelr::rmse(lm2.Home.score, data = test.Home.score)

predict(lm2.Home.score, newdata = Predictions.test)


lm1.Visitor.score <- lm(Visitor.Score ~ ., data = train.Visitor.score)
step.visitor.score <- step(lm1.Visitor.score, trace = 0)

lm2.Visitor.score <- lm(formula(step.visitor.score), data = train.Visitor.score)
summary(lm2.Visitor.score)
predict(lm2.Visitor.score, newdata = test.Visitor.score)
modelr::rmse(lm2.Visitor.score, data = test.Visitor.score)

predict(lm2.Visitor.score, newdata = Predictions.test)
```

Poisson
```{r}
poisson1.home.score <- glm(Home.Score ~., data = train.Home.score, family = poisson) 
step.poisson1.home.score <- step(poisson1.home.score, trace =0)

poisson2.home.score <- glm(formula = step.poisson1.home.score, data = train.Home.score)
summary(poisson2.home.score)
predict(poisson2.home.score, newdata = Predictions.test)
modelr::rmse(poisson2.home.score, data = train.Home.score)

poisson1.visitor.score <- glm(Visitor.Score ~., data = train.Visitor.score, family = poisson)
step.poisson1.home.score <- step(poisson1.visitor.score, trace =0)

poisson2.visitor.score <- glm(formula = step.poisson1.home.score, data = train.Visitor.score)
summary(poisson2.visitor.score)
predict(poisson2.visitor.score, newdata = Predictions.test)
modelr::rmse(poisson2.visitor.score, data = test.Visitor.score)
```

Lasso
```{r}
lasso.train.Home.score <- train.Home.score[, -c(1, 2)]
lasso.train.Home.score <- as.data.frame(sapply(lasso.train.Home.score, as.numeric))
lasso.test.Home.score <- test.Home.score[, -c(1, 2)]
lasso.test.Home.score <- as.data.frame(sapply(lasso.test.Home.score, as.numeric))

set.seed(1305)
x.train.home <- model.matrix(Home.Score~., lasso.train.Home.score)[, -1]
y.train.home <- lasso.train.Home.score$Home.Score
x.test.home <- model.matrix(Home.Score~., lasso.test.Home.score)[, -1]
y.test.home <- lasso.test.Home.score$Home.Score
lasso1.home.score = cv.glmnet(x.train.home, y.train.home, family = "poisson", alpha = 1)
# plot(optimal.lam.home)
# plot(lasso1.home.score)
coef(lasso1.home.score, s='lambda.min')
# summary(lasso1.home.score)
predict(lasso1.home.score, newx = data.matrix(Predictions.test[, -c(1:7)]), type = "response", s = 'lambda.min')


lasso.train.Visitor.score <- train.Visitor.score[, -c(1, 2)]
lasso.train.Visitor.score <- as.data.frame(sapply(lasso.train.Visitor.score, as.numeric))
lasso.test.Visitor.score <- test.Visitor.score[, -c(1, 2)]
lasso.test.Visitor.score <- as.data.frame(sapply(lasso.test.Visitor.score, as.numeric))

set.seed(1305)
x.train.visitor <- model.matrix(Visitor.Score~., lasso.train.Visitor.score)[, -1]
y.train.visitor <- lasso.train.Visitor.score$Visitor.Score
x.test.visitor <- model.matrix(Visitor.Score~., lasso.test.Visitor.score)[, -1]
y.test.visitor <- lasso.test.Visitor.score$Visitor.Score
lasso1.visitor.score = cv.glmnet(x.train.visitor, y.train.visitor, family = "poisson", alpha = 1)
# plot(optimal.lam.home)
# plot(lasso1.home.score)
coef(lasso1.visitor.score, s='lambda.min')
# summary(lasso1.home.score)
predict(lasso1.visitor.score, newx = data.matrix(Predictions.test[, -c(1:7)]), type = "response", s = 'lambda.min')

sqrt(lasso1.home.score$cvm[lasso1.home.score$lambda == lasso1.home.score$lambda.1se])
sqrt(lasso1.visitor.score$cvm[lasso1.visitor.score$lambda == lasso1.visitor.score$lambda.1se])
```

Random Forest
```{r}
#randomForest(Home.Score ~ ., train.score[, c(3:20, 41, 43, 44, 47)], type = "regression")

oob.err=double(18)
test.err=double(18)

#mtry is no of Variables randomly chosen at each split
for(mtry in 1:18) {
  rf1.home.score=randomForest(Home.Score ~ ., train.Home.score[, c(3:45)], type = "regression", mtry=mtry, ntree=400) 
  oob.err[mtry] = rf1.home.score$mse[400] #Error of all Trees fitted
  
  pred<-predict(rf1.home.score, test.Home.score[, c(3:45)]) #Predictions on Test Set for each Tree
  test.err[mtry]= with(test.Home.score[, c(3:45)], mean( (Home.Score - pred)^2)) #Mean Squared Test Error
  
  cat(mtry," ") #printing the output to the console
  
}
which.min(test.err)
#test.err

predict(rf1.home.score, Predictions.test)
importance(rf1.home.score)


for(mtry in 1:18) {
  rf1.visitor.score=randomForest(Visitor.Score ~ ., train.Visitor.score[, c(3:45)], type = "regression", mtry=mtry, ntree=400) 
  oob.err[mtry] = rf1.visitor.score$mse[400] #Error of all Trees fitted
  
  pred<-predict(rf1.visitor.score, test.Visitor.score[, c(3:45)]) #Predictions on Test Set for each Tree
  test.err[mtry]= with(test.Visitor.score[, c(3:45)], mean( (Visitor.Score - pred)^2)) #Mean Squared Test Error
  
  cat(mtry," ") #printing the output to the console
  
}
which.min(test.err)
#test.err

predict(rf1.home.score, Predictions.test)
predict(rf1.visitor.score, Predictions.test)

importance(rf1.visitor.score)
```







```{r}
# OStats19 <- read_csv("C:/Users/barlo/Downloads/2019 Offense.csv", 
#     skip = 1)
# OStats19 <- OStats19[, c(1,2)]
# 
# DStats19 <- read_csv("C:/Users/barlo/Downloads/2019 Defense.csv", 
#     skip = 1)
# DStats19 <- DStats19[, c(1,2)]
# 
# rdf <- left_join(OStats19, DStats19, by = "School")
# names(rdf) <- c("O.Rank", "School", "D.Rank")
# write.csv(rdf, file = "Ranking.csv")
```

